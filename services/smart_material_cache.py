#!/usr/bin/env python3
"""
æ™ºèƒ½ç´ æç¼“å­˜ç³»ç»Ÿ
æ”¯æŒæœ¬åœ°ç¼“å­˜ + OSSå¤‡ä»½ + å»é‡ä¸Šä¼ 
"""
import os
import hashlib
import json
import time
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from models.oss_client import OSSClient
import asyncio

class SmartMaterialCache:
    """æ™ºèƒ½ç´ æç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.cache_dir = "cache/materials"
        self.metadata_dir = "cache/metadata"
        self.cache_index_file = "cache/cache_index.json"
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(self.metadata_dir, exist_ok=True)
        
        # ç¼“å­˜é…ç½®
        self.max_cache_size_gb = 10  # æœ€å¤§ç¼“å­˜10GB
        self.cache_expire_days = 7   # ç¼“å­˜è¿‡æœŸæ—¶é—´7å¤©
        
        # OSSå®¢æˆ·ç«¯
        self.oss_client = OSSClient()
        
        # åŠ è½½ç¼“å­˜ç´¢å¼•
        self.cache_index = self._load_cache_index()
    
    async def get_material(self, url: str, material_type: str = "video") -> str:
        """
        æ™ºèƒ½è·å–ç´ ææ–‡ä»¶
        
        Args:
            url: ç´ æURL
            material_type: ç´ æç±»å‹ (video/audio/poster)
        
        Returns:
            æœ¬åœ°æ–‡ä»¶è·¯å¾„
        """
        # å®šæœŸæ¸…ç†ç¼“å­˜ï¼ˆæ¯æ¬¡è®¿é—®æ—¶æ£€æŸ¥ï¼Œé¿å…ç¼“å­˜æ— é™å¢é•¿ï¼‰
        await self._auto_cleanup()
        
        # å›¢é˜Ÿåä½œæ¨¡å¼ï¼šåªå¤„ç†ç¼“å­˜ä¸­çš„æ–‡ä»¶å’ŒHTTP URL
        if url.startswith("cache/") and os.path.exists(url):
            print(f"ğŸ“ ç›´æ¥ä½¿ç”¨ç¼“å­˜æ–‡ä»¶: {url}")
            return url
        
        # 1. è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼ˆç”¨äºå»é‡ï¼‰
        file_hash = self._calculate_url_hash(url)
        
        # 2. æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        local_path = await self._check_local_cache(file_hash, url, material_type)
        if local_path and os.path.exists(local_path):
            print(f"âœ… ç¼“å­˜å‘½ä¸­: {url[:50]}... -> {local_path}")
            self._update_access_time(file_hash)
            return local_path
        
        # 3. ä¸‹è½½å¹¶ç¼“å­˜æ–‡ä»¶
        print(f"ğŸ“¥ ä¸‹è½½ç´ æ: {url[:50]}...")
        local_path = await self._download_and_cache(url, file_hash, material_type)
        
        return local_path
    
    async def upload_material(self, file_buffer: bytes, original_filename: str, folder: str = "uploads") -> Tuple[str, str]:
        """
        æ™ºèƒ½ä¸Šä¼ ç´ æï¼ˆå»é‡ï¼‰
        
        Args:
            file_buffer: æ–‡ä»¶å†…å®¹
            original_filename: åŸå§‹æ–‡ä»¶å
            folder: OSSæ–‡ä»¶å¤¹
        
        Returns:
            (OSS_URL, file_hash)
        """
        # 1. è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        file_hash = self._calculate_content_hash(file_buffer)
        
        # 2. æ£€æŸ¥æ˜¯å¦å·²ç»ä¸Šä¼ è¿‡ç›¸åŒæ–‡ä»¶
        existing_url = self._check_uploaded_file(file_hash)
        if existing_url:
            print(f"âœ… æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ : {existing_url[:50]}...")
            return existing_url, file_hash
        
        # 3. ä¸Šä¼ åˆ°OSS
        print(f"ğŸ“¤ ä¸Šä¼ æ–°æ–‡ä»¶: {original_filename}")
        oss_url = await self.oss_client.upload_to_oss(
            file_buffer=file_buffer,
            original_filename=original_filename,
            folder=folder
        )
        
        # 4. è®°å½•ä¸Šä¼ ä¿¡æ¯
        self._record_uploaded_file(file_hash, oss_url, original_filename)
        
        return oss_url, file_hash
    
    async def preload_materials(self, urls: List[str]) -> Dict[str, str]:
        """
        æ‰¹é‡é¢„åŠ è½½ç´ æï¼ˆå¹¶è¡Œä¸‹è½½ï¼‰
        
        Args:
            urls: ç´ æURLåˆ—è¡¨
        
        Returns:
            URLåˆ°æœ¬åœ°è·¯å¾„çš„æ˜ å°„
        """
        print(f"ğŸš€ å¼€å§‹ä¼˜åŒ–æ‰¹é‡é¢„åŠ è½½ {len(urls)} ä¸ªç´ æ...")
        
        # ğŸš€ ä¼˜åŒ–: é™åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…CPUè¿‡è½½
        max_concurrent = 3  # æœ€å¤š3ä¸ªå¹¶å‘ä¸‹è½½ï¼Œé¿å…CPUå’Œç½‘ç»œè¿‡è½½
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def download_with_semaphore(url):
            async with semaphore:
                material_type = self._guess_material_type(url)
                return await self.get_material(url, material_type)
        
        # å¹¶è¡Œä¸‹è½½ï¼ˆå—æ§å¹¶å‘ï¼‰
        tasks = []
        for url in urls:
            task = asyncio.create_task(download_with_semaphore(url))
            tasks.append(task)
        
        print(f"   ä½¿ç”¨{max_concurrent}ä¸ªå¹¶å‘è¿æ¥ä¸‹è½½ï¼Œå‡å°‘CPUè´Ÿè½½")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ„å»ºç»“æœæ˜ å°„
        url_to_path = {}
        for i, (url, result) in enumerate(zip(urls, results)):
            if isinstance(result, Exception):
                print(f"âŒ é¢„åŠ è½½å¤±è´¥: {url[:50]}... - {result}")
            else:
                url_to_path[url] = result
                print(f"âœ… é¢„åŠ è½½å®Œæˆ: {url[:50]}... -> {result}")
        
        print(f"ğŸ“Š é¢„åŠ è½½ç»Ÿè®¡: æˆåŠŸ {len(url_to_path)}/{len(urls)}")
        return url_to_path
    
    def _calculate_url_hash(self, url: str) -> str:
        """è®¡ç®—URLå“ˆå¸Œ"""
        return hashlib.md5(url.encode('utf-8')).hexdigest()
    
    def _calculate_content_hash(self, content: bytes) -> str:
        """è®¡ç®—æ–‡ä»¶å†…å®¹å“ˆå¸Œ"""
        return hashlib.sha256(content).hexdigest()
    
    async def _check_local_cache(self, file_hash: str, url: str, material_type: str) -> Optional[str]:
        """æ£€æŸ¥æœ¬åœ°ç¼“å­˜"""
        if file_hash in self.cache_index:
            cache_info = self.cache_index[file_hash]
            local_path = cache_info['local_path']
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(local_path):
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                cache_time = datetime.fromisoformat(cache_info['cached_at'])
                if datetime.now() - cache_time < timedelta(days=self.cache_expire_days):
                    return local_path
                else:
                    # è¿‡æœŸåˆ é™¤
                    self._remove_from_cache(file_hash)
        
        return None
    
    async def _download_and_cache(self, url: str, file_hash: str, material_type: str) -> str:
        """ä¸‹è½½å¹¶ç¼“å­˜æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«éªŒè¯ï¼‰"""
        # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶è·¯å¾„
        file_extension = self._get_file_extension(url)
        local_filename = f"{file_hash}{file_extension}"
        local_path = os.path.join(self.cache_dir, local_filename)

        # ä¸‹è½½æ–‡ä»¶
        try:
            # å¯¹äºè§†é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å¢å¼ºä¸‹è½½å™¨
            if material_type == "video" or file_extension.lower() in ['.mp4', '.avi', '.mov', '.mkv', '.webm']:
                from .enhanced_video_downloader import download_and_validate_video

                print(f"ğŸ“¥ ä½¿ç”¨GPUä¼˜åŒ–ä¸‹è½½å™¨ä¸‹è½½è§†é¢‘: {url[:50]}...")
                # ğŸš€ ä¼˜åŒ–: åœ¨ä¸‹è½½é˜¶æ®µå‡å°‘éªŒè¯å¼ºåº¦ï¼ŒåŠ å¿«ä¸‹è½½é€Ÿåº¦
                success = await download_and_validate_video(url, local_path, 
                                                          skip_deep_validation=True)  # è·³è¿‡æ·±åº¦éªŒè¯

                if not success:
                    raise Exception(f"ä¼˜åŒ–ä¸‹è½½å™¨ä¸‹è½½å¤±è´¥: {url}")

            else:
                # éè§†é¢‘æ–‡ä»¶ä½¿ç”¨åŸæœ‰æ–¹æ³•
                await self.oss_client.download_video(url, local_path)

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æˆåŠŸä¸‹è½½
                if not os.path.exists(local_path) or os.path.getsize(local_path) == 0:
                    raise Exception(f"ä¸‹è½½çš„æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨: {local_path}")

            # è®°å½•åˆ°ç¼“å­˜ç´¢å¼•
            self._add_to_cache(file_hash, url, local_path, material_type)

            print(f"âœ… ä¸‹è½½å®Œæˆ: {local_path} ({os.path.getsize(local_path)/(1024*1024):.1f}MB)")
            return local_path
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {url} - {e}")
            # æ¸…ç†å¯èƒ½çš„ä¸å®Œæ•´æ–‡ä»¶
            if os.path.exists(local_path):
                os.remove(local_path)
            raise e

    async def validate_cached_video(self, local_path: str) -> bool:
        """éªŒè¯ç¼“å­˜çš„è§†é¢‘æ–‡ä»¶"""
        try:
            from .enhanced_video_downloader import validate_existing_video
            return await validate_existing_video(local_path)
        except Exception as e:
            print(f"âŒ è§†é¢‘éªŒè¯å¤±è´¥: {e}")
            return False

    async def repair_cached_video(self, local_path: str) -> bool:
        """ä¿®å¤ç¼“å­˜çš„è§†é¢‘æ–‡ä»¶"""
        try:
            from .enhanced_video_downloader import repair_video

            # åˆ›å»ºä¿®å¤åçš„æ–‡ä»¶è·¯å¾„
            repair_path = local_path.replace('.mp4', '_repaired.mp4')

            success = await repair_video(local_path, repair_path)

            if success:
                # æ›¿æ¢åŸæ–‡ä»¶
                import shutil
                shutil.move(repair_path, local_path)
                print(f"âœ… è§†é¢‘æ–‡ä»¶ä¿®å¤æˆåŠŸ: {local_path}")
                return True
            else:
                # æ¸…ç†ä¿®å¤å¤±è´¥çš„æ–‡ä»¶
                if os.path.exists(repair_path):
                    os.remove(repair_path)
                return False

        except Exception as e:
            print(f"âŒ è§†é¢‘ä¿®å¤å¤±è´¥: {e}")
            return False

    async def cleanup_corrupted_files(self):
        """æ¸…ç†æŸåçš„ç¼“å­˜æ–‡ä»¶"""
        print("ğŸ” æ£€æŸ¥å¹¶æ¸…ç†æŸåçš„è§†é¢‘æ–‡ä»¶...")

        corrupted_files = []
        total_checked = 0

        for file_hash, cache_info in list(self.cache_index.items()):
            local_path = cache_info['local_path']

            if not os.path.exists(local_path):
                continue

            # åªæ£€æŸ¥è§†é¢‘æ–‡ä»¶
            if not local_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.webm')):
                continue

            total_checked += 1
            print(f"ğŸ” æ£€æŸ¥æ–‡ä»¶ {total_checked}: {os.path.basename(local_path)}")

            # éªŒè¯è§†é¢‘æ–‡ä»¶
            is_valid = await self.validate_cached_video(local_path)

            if not is_valid:
                print(f"âŒ å‘ç°æŸåæ–‡ä»¶: {local_path}")
                corrupted_files.append((file_hash, local_path))

        # æ¸…ç†æŸåçš„æ–‡ä»¶
        if corrupted_files:
            print(f"ğŸ—‘ï¸ æ¸…ç† {len(corrupted_files)} ä¸ªæŸåæ–‡ä»¶...")

            for file_hash, local_path in corrupted_files:
                try:
                    # ä»ç¼“å­˜ç´¢å¼•ä¸­ç§»é™¤
                    if file_hash in self.cache_index:
                        del self.cache_index[file_hash]

                    # åˆ é™¤æ–‡ä»¶
                    if os.path.exists(local_path):
                        os.remove(local_path)

                    print(f"   ğŸ—‘ï¸ å·²åˆ é™¤: {os.path.basename(local_path)}")

                except Exception as e:
                    print(f"   âŒ åˆ é™¤å¤±è´¥: {local_path} - {e}")

            # ä¿å­˜æ›´æ–°çš„ç´¢å¼•
            self._save_cache_index()

            print(f"âœ… æŸåæ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {len(corrupted_files)} ä¸ªæ–‡ä»¶")
        else:
            print(f"âœ… æ£€æŸ¥å®Œæˆï¼Œæœªå‘ç°æŸåæ–‡ä»¶ (å…±æ£€æŸ¥ {total_checked} ä¸ªè§†é¢‘æ–‡ä»¶)")
    
    def _get_file_extension(self, url: str) -> str:
        """ä»URLè·å–æ–‡ä»¶æ‰©å±•å"""
        filename = url.split('/')[-1].split('?')[0]  # ç§»é™¤æŸ¥è¯¢å‚æ•°
        if '.' in filename:
            return '.' + filename.split('.')[-1]
        else:
            # æ ¹æ®URLçŒœæµ‹ç±»å‹
            if 'video' in url.lower():
                return '.mp4'
            elif 'audio' in url.lower():
                return '.mp3'
            elif 'image' in url.lower() or 'poster' in url.lower():
                return '.jpg'
            else:
                return '.tmp'
    
    def _guess_material_type(self, url: str) -> str:
        """æ ¹æ®URLçŒœæµ‹ç´ æç±»å‹"""
        url_lower = url.lower()
        if any(ext in url_lower for ext in ['.mp4', '.avi', '.mov', 'video']):
            return 'video'
        elif any(ext in url_lower for ext in ['.mp3', '.wav', '.aac', 'audio']):
            return 'audio'
        elif any(ext in url_lower for ext in ['.jpg', '.png', '.jpeg', 'image', 'poster']):
            return 'poster'
        else:
            return 'unknown'
    
    def _add_to_cache(self, file_hash: str, url: str, local_path: str, material_type: str):
        """æ·»åŠ åˆ°ç¼“å­˜ç´¢å¼•"""
        self.cache_index[file_hash] = {
            'url': url,
            'local_path': local_path,
            'material_type': material_type,
            'cached_at': datetime.now().isoformat(),
            'last_accessed': datetime.now().isoformat(),
            'file_size': os.path.getsize(local_path) if os.path.exists(local_path) else 0
        }
        self._save_cache_index()
    
    def _update_access_time(self, file_hash: str):
        """æ›´æ–°è®¿é—®æ—¶é—´"""
        if file_hash in self.cache_index:
            self.cache_index[file_hash]['last_accessed'] = datetime.now().isoformat()
            self._save_cache_index()
    
    def _remove_from_cache(self, file_hash: str):
        """ä»ç¼“å­˜ä¸­ç§»é™¤"""
        if file_hash in self.cache_index:
            cache_info = self.cache_index[file_hash]
            local_path = cache_info['local_path']
            
            # åˆ é™¤æœ¬åœ°æ–‡ä»¶
            if os.path.exists(local_path):
                os.remove(local_path)
            
            # ä»ç´¢å¼•ä¸­ç§»é™¤
            del self.cache_index[file_hash]
            self._save_cache_index()
    
    def _load_cache_index(self) -> Dict:
        """åŠ è½½ç¼“å­˜ç´¢å¼•"""
        if os.path.exists(self.cache_index_file):
            try:
                with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"åŠ è½½ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
        return {}
    
    def _save_cache_index(self):
        """ä¿å­˜ç¼“å­˜ç´¢å¼•"""
        try:
            with open(self.cache_index_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
    
    def _check_uploaded_file(self, file_hash: str) -> Optional[str]:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ä¸Šä¼ """
        # æ£€æŸ¥ä¸Šä¼ è®°å½•æ–‡ä»¶
        upload_record_file = os.path.join(self.metadata_dir, "upload_records.json")
        if os.path.exists(upload_record_file):
            try:
                with open(upload_record_file, 'r', encoding='utf-8') as f:
                    upload_records = json.load(f)
                    if file_hash in upload_records:
                        return upload_records[file_hash]['oss_url']
            except Exception as e:
                print(f"è¯»å–ä¸Šä¼ è®°å½•å¤±è´¥: {e}")
        return None
    
    def _record_uploaded_file(self, file_hash: str, oss_url: str, filename: str):
        """è®°å½•å·²ä¸Šä¼ æ–‡ä»¶"""
        upload_record_file = os.path.join(self.metadata_dir, "upload_records.json")
        
        # åŠ è½½ç°æœ‰è®°å½•
        upload_records = {}
        if os.path.exists(upload_record_file):
            try:
                with open(upload_record_file, 'r', encoding='utf-8') as f:
                    upload_records = json.load(f)
            except Exception:
                pass
        
        # æ·»åŠ æ–°è®°å½•
        upload_records[file_hash] = {
            'oss_url': oss_url,
            'filename': filename,
            'uploaded_at': datetime.now().isoformat()
        }
        
        # ä¿å­˜è®°å½•
        try:
            with open(upload_record_file, 'w', encoding='utf-8') as f:
                json.dump(upload_records, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜ä¸Šä¼ è®°å½•å¤±è´¥: {e}")
    
    def cleanup_cache(self, force: bool = False):
        """æ¸…ç†ç¼“å­˜"""
        print("ğŸ§¹ å¼€å§‹æ¸…ç†ç¼“å­˜...")
        
        current_time = datetime.now()
        total_size = 0
        expired_files = []
        
        # æ£€æŸ¥è¿‡æœŸå’Œè®¡ç®—æ€»å¤§å°
        for file_hash, cache_info in list(self.cache_index.items()):
            local_path = cache_info['local_path']
            
            if not os.path.exists(local_path):
                # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»ç´¢å¼•ç§»é™¤
                del self.cache_index[file_hash]
                continue
            
            file_size = os.path.getsize(local_path)
            total_size += file_size
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            cached_time = datetime.fromisoformat(cache_info['cached_at'])
            if current_time - cached_time > timedelta(days=self.cache_expire_days) or force:
                expired_files.append((file_hash, local_path, file_size))
        
        # åˆ é™¤è¿‡æœŸæ–‡ä»¶
        for file_hash, local_path, file_size in expired_files:
            try:
                os.remove(local_path)
                del self.cache_index[file_hash]
                total_size -= file_size
                print(f"   åˆ é™¤è¿‡æœŸæ–‡ä»¶: {local_path}")
            except Exception as e:
                print(f"   åˆ é™¤å¤±è´¥: {local_path} - {e}")
        
        # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
        max_size_bytes = self.max_cache_size_gb * 1024 * 1024 * 1024
        if total_size > max_size_bytes:
            print(f"âš ï¸  ç¼“å­˜è¶…å‡ºé™åˆ¶ ({total_size/(1024**3):.1f}GB > {self.max_cache_size_gb}GB)")
            self._cleanup_by_lru(total_size - max_size_bytes)
        
        self._save_cache_index()
        
        final_size = sum(
            os.path.getsize(info['local_path']) 
            for info in self.cache_index.values() 
            if os.path.exists(info['local_path'])
        )
        
        print(f"âœ… ç¼“å­˜æ¸…ç†å®Œæˆ:")
        print(f"   æ–‡ä»¶æ•°é‡: {len(self.cache_index)}")
        print(f"   ç¼“å­˜å¤§å°: {final_size/(1024**3):.2f}GB")
        print(f"   åˆ é™¤æ–‡ä»¶: {len(expired_files)}ä¸ª")
    
    def _cleanup_by_lru(self, bytes_to_free: int):
        """æ ¹æ®LRUç­–ç•¥æ¸…ç†ç¼“å­˜"""
        print(f"ğŸ”„ å¯åŠ¨LRUæ¸…ç†ï¼Œéœ€è¦é‡Šæ”¾ {bytes_to_free/(1024**2):.1f}MB")
        
        # æŒ‰æœ€åè®¿é—®æ—¶é—´æ’åº
        sorted_items = sorted(
            self.cache_index.items(),
            key=lambda x: x[1]['last_accessed']
        )
        
        freed_bytes = 0
        for file_hash, cache_info in sorted_items:
            if freed_bytes >= bytes_to_free:
                break
            
            local_path = cache_info['local_path']
            if os.path.exists(local_path):
                file_size = os.path.getsize(local_path)
                try:
                    os.remove(local_path)
                    del self.cache_index[file_hash]
                    freed_bytes += file_size
                    print(f"   LRUåˆ é™¤: {local_path}")
                except Exception as e:
                    print(f"   LRUåˆ é™¤å¤±è´¥: {local_path} - {e}")
        
        print(f"âœ… LRUæ¸…ç†å®Œæˆï¼Œé‡Šæ”¾äº† {freed_bytes/(1024**2):.1f}MB")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_files = len(self.cache_index)
        total_size = 0
        by_type = {"video": 0, "audio": 0, "poster": 0, "unknown": 0}
        
        for cache_info in self.cache_index.values():
            if os.path.exists(cache_info['local_path']):
                total_size += cache_info['file_size']
                material_type = cache_info['material_type']
                by_type[material_type] = by_type.get(material_type, 0) + 1
        
        return {
            "total_files": total_files,
            "total_size_gb": total_size / (1024**3),
            "by_type": by_type,
            "cache_dir": self.cache_dir,
            "max_size_gb": self.max_cache_size_gb,
            "expire_days": self.cache_expire_days
        }
    
    async def _auto_cleanup(self):
        """è‡ªåŠ¨æ¸…ç†ç¼“å­˜ - æ™ºèƒ½è§¦å‘æœºåˆ¶"""
        import time
        
        # ä½¿ç”¨æ–‡ä»¶è®°å½•ä¸Šæ¬¡æ¸…ç†æ—¶é—´ï¼Œé¿å…é¢‘ç¹æ¸…ç†
        cleanup_flag_file = os.path.join(self.metadata_dir, "last_cleanup.txt")
        
        try:
            # æ£€æŸ¥ä¸Šæ¬¡æ¸…ç†æ—¶é—´
            if os.path.exists(cleanup_flag_file):
                with open(cleanup_flag_file, 'r') as f:
                    last_cleanup_str = f.read().strip()
                last_cleanup = datetime.fromisoformat(last_cleanup_str)
                
                # å¦‚æœè·ç¦»ä¸Šæ¬¡æ¸…ç†ä¸åˆ°1å°æ—¶ï¼Œè·³è¿‡
                if datetime.now() - last_cleanup < timedelta(hours=1):
                    return
            
            # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
            stats = self.get_cache_stats()
            need_cleanup = False
            
            # æ¡ä»¶1ï¼šç¼“å­˜è¶…è¿‡5GBæ—¶è§¦å‘
            if stats["total_size_gb"] > 5:
                print(f"ğŸ§¹ ç¼“å­˜å¤§å° {stats['total_size_gb']:.1f}GB > 5GBï¼Œè§¦å‘æ¸…ç†")
                need_cleanup = True
            
            # æ¡ä»¶2ï¼šæ–‡ä»¶æ•°è¶…è¿‡50ä¸ªæ—¶è§¦å‘
            elif stats["total_files"] > 50:
                print(f"ğŸ§¹ ç¼“å­˜æ–‡ä»¶ {stats['total_files']} ä¸ª > 50ä¸ªï¼Œè§¦å‘æ¸…ç†")
                need_cleanup = True
            
            # æ¡ä»¶3ï¼šè·ç¦»ä¸Šæ¬¡æ¸…ç†è¶…è¿‡24å°æ—¶æ—¶è§¦å‘
            elif not os.path.exists(cleanup_flag_file):
                print("ğŸ§¹ é¦–æ¬¡å¯åŠ¨æˆ–è¶…è¿‡24å°æ—¶æœªæ¸…ç†ï¼Œè§¦å‘æ¸…ç†")
                need_cleanup = True
            else:
                # æ¯24å°æ—¶å¼ºåˆ¶æ¸…ç†ä¸€æ¬¡
                if datetime.now() - last_cleanup > timedelta(hours=24):
                    print("ğŸ§¹ è·ç¦»ä¸Šæ¬¡æ¸…ç†è¶…è¿‡24å°æ—¶ï¼Œè§¦å‘æ¸…ç†")
                    need_cleanup = True
            
            if need_cleanup:
                # æ‰§è¡Œæ¸…ç†
                self.cleanup_cache()
                
                # è®°å½•æ¸…ç†æ—¶é—´
                with open(cleanup_flag_file, 'w') as f:
                    f.write(datetime.now().isoformat())
                
        except Exception as e:
            print(f"âš ï¸ è‡ªåŠ¨æ¸…ç†æ£€æŸ¥å¤±è´¥: {e}")

# å…¨å±€å®ä¾‹
smart_cache = SmartMaterialCache()
