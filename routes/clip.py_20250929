import asyncio
from fastapi import APIRouter, BackgroundTasks
from pydantic import BaseModel
from typing import List, Dict, Any, Set, Optional
from services.clip_service import process_clips001
from services.gpu_task_scheduler import gpu_scheduler, GPUTask, TaskType, TaskPriority
from services.performance_optimizer import performance_optimizer
from uuid import uuid4
from datetime import datetime
import time

router = APIRouter()

class VideoFile(BaseModel):
    id: str
    name: str
    url: str
    size: int
    duration: int
    uploadedAt: str
    thumbnail: Optional[str] = None

class AudioFile(BaseModel):
    id: str
    name: str
    url: str
    size: int
    duration: int
    uploadedAt: str
    thumbnail: Optional[str] = None

class PosterFile(BaseModel):
    id: str
    name: str
    url: str
    size: int
    uploadedAt: str

class Script(BaseModel):
    id: str
    content: str
    selected: bool
    generatedAt: str

class StyleConfig(BaseModel):
    title: Dict[str, Any]
    subtitle: Dict[str, Any]

class ClipRequest(BaseModel):
    name: str
    videos: List[VideoFile]
    audios: List[AudioFile]
    posters: Optional[List[PosterFile]] = None
    scripts: List[Script]
    duration: str
    videoCount: int
    voice: str
    voiceSpeed: Optional[float] = 1.0
    style: StyleConfig
    portraitMode: Optional[bool] = None

class StartGenerationRequest(BaseModel):
    projectId: str

# æ·»åŠ å…¨å±€å˜é‡å­˜å‚¨æœ€æ–°ç”Ÿæˆçš„è§†é¢‘
_latest_generated_videos = []

# ä»»åŠ¡çŠ¶æ€å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®ç”¨Redisï¼‰
_task_storage = {}

# æ·»åŠ é¡¹ç›®å­˜å‚¨
_project_storage = {}

# ğŸš€ GPUä¼˜åŒ–ï¼šæ”¯æŒæ›´å¤šå¹¶å‘ä»»åŠ¡å……åˆ†åˆ©ç”¨RTX 3090
_task_queue = asyncio.Queue()  # ä»»åŠ¡é˜Ÿåˆ—
_processing_tasks: Set[str] = set()  # æ­£åœ¨å¤„ç†çš„ä»»åŠ¡IDé›†åˆ
_max_concurrent_tasks = 5  # å¢åŠ å¹¶å‘æ•°ä»¥å……åˆ†åˆ©ç”¨GPU (RTX 3090å¯æ”¯æŒæ›´å¤šå¹¶å‘)
_queue_processor_started = False  # é˜Ÿåˆ—å¤„ç†å™¨æ˜¯å¦å·²å¯åŠ¨
_gpu_optimization_enabled = True  # GPUä¼˜åŒ–å¼€å…³

async def start_concurrent_queue_processor():
    """å¯åŠ¨GPUä¼˜åŒ–çš„å¹¶å‘é˜Ÿåˆ—å¤„ç†å™¨ - æ”¯æŒæ›´å¤šå¹¶å‘ä»»åŠ¡"""
    global _processing_tasks, _queue_processor_started

    if _queue_processor_started:
        return

    _queue_processor_started = True
    print(f"ğŸš€ å¯åŠ¨GPUä¼˜åŒ–å¹¶å‘é˜Ÿåˆ—å¤„ç†å™¨ (æ”¯æŒ{_max_concurrent_tasks}ä¸ªå¹¶å‘ä»»åŠ¡)...")

    # å¯åŠ¨GPUè°ƒåº¦å™¨å’Œæ€§èƒ½ç›‘æ§
    if _gpu_optimization_enabled:
        try:
            await gpu_scheduler.start()
            await performance_optimizer.start_monitoring()
            print("âœ… GPUè°ƒåº¦å™¨å’Œæ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
        except Exception as e:
            print(f"âš ï¸ GPUä¼˜åŒ–å¯åŠ¨å¤±è´¥ï¼Œä½¿ç”¨CPUæ¨¡å¼: {e}")

    # åˆ›å»ºæ›´å¤šå¹¶å‘å·¥ä½œåç¨‹ä»¥å……åˆ†åˆ©ç”¨GPU
    workers = []
    for i in range(_max_concurrent_tasks):
        worker = asyncio.create_task(gpu_optimized_worker_coroutine(f"GPU-Worker-{i+1}"))
        workers.append(worker)

    # ç­‰å¾…æ‰€æœ‰å·¥ä½œåç¨‹å®Œæˆ
    await asyncio.gather(*workers)

async def gpu_optimized_worker_coroutine(worker_name: str):
    """GPUä¼˜åŒ–çš„å·¥ä½œåç¨‹ - æ™ºèƒ½èµ„æºåˆ†é…å’Œä»»åŠ¡å¤„ç†"""
    print(f"ğŸš€ {worker_name} å·²å¯åŠ¨ (GPUä¼˜åŒ–æ¨¡å¼)")

    while True:
        try:
            # ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡
            task_data = await _task_queue.get()
            task_id = task_data['task_id']
            clip_req = task_data['clip_req']

            # å°†ä»»åŠ¡æ·»åŠ åˆ°å¤„ç†é›†åˆ
            _processing_tasks.add(task_id)

            print(f"ğŸ“‹ {worker_name} å¼€å§‹GPUåŠ é€Ÿå¤„ç†: {task_id}")
            print(f"ğŸ”„ å½“å‰å¹¶å‘ä»»åŠ¡æ•°: {len(_processing_tasks)}/{_max_concurrent_tasks}")

            # æ£€æŸ¥GPUçŠ¶æ€
            if _gpu_optimization_enabled:
                gpu_status = gpu_scheduler.get_status()
                print(f"   GPUçŠ¶æ€: {gpu_status['gpu_tasks_running']}ä¸ªGPUä»»åŠ¡è¿è¡Œä¸­")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
            if task_id in _task_storage:
                _task_storage[task_id]["status"] = "processing"
                _task_storage[task_id]["progress"] = 10
                _task_storage[task_id]["updatedAt"] = datetime.now().isoformat()
                _task_storage[task_id]["worker"] = worker_name
                _task_storage[task_id]["gpu_enabled"] = _gpu_optimization_enabled

            # æ‰§è¡ŒGPUä¼˜åŒ–çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡
            start_time = time.time()
            await process_video_generation(task_id, clip_req)
            processing_time = time.time() - start_time

            print(f"âœ… {worker_name} å®Œæˆä»»åŠ¡: {task_id} (è€—æ—¶: {processing_time:.1f}ç§’)")

            # è®°å½•æ€§èƒ½ç»Ÿè®¡
            if task_id in _task_storage:
                _task_storage[task_id]["processing_time"] = processing_time
                _task_storage[task_id]["performance_optimized"] = True

        except Exception as e:
            print(f"âŒ {worker_name} å¤„ç†ä»»åŠ¡å¼‚å¸¸: {e}")
            if task_id in _task_storage:
                _task_storage[task_id]["status"] = "failed"
                _task_storage[task_id]["error"] = f"GPUä¼˜åŒ–å¤„ç†å¼‚å¸¸: {str(e)}"
                _task_storage[task_id]["updatedAt"] = datetime.now().isoformat()
        finally:
            # ä»å¤„ç†é›†åˆä¸­ç§»é™¤ä»»åŠ¡
            if task_id in _processing_tasks:
                _processing_tasks.remove(task_id)
            _task_queue.task_done()
            print(f"ğŸ”„ å½“å‰å¹¶å‘ä»»åŠ¡æ•°: {len(_processing_tasks)}/{_max_concurrent_tasks}")

# ä¿æŒåŸæœ‰çš„worker_coroutineä½œä¸ºå¤‡ç”¨
async def worker_coroutine(worker_name: str):
    """ä¼ ç»Ÿå·¥ä½œåç¨‹ - å¤‡ç”¨æ¨¡å¼"""
    return await gpu_optimized_worker_coroutine(worker_name)

async def process_clips_with_gpu_acceleration(task_id: str, clip_req: ClipRequest):
    """GPUåŠ é€Ÿçš„è§†é¢‘å¤„ç†æ ¸å¿ƒå‡½æ•°"""
    try:
        print(f"ğŸ¬ å¯åŠ¨GPUåŠ é€Ÿè§†é¢‘å¤„ç†: {clip_req.name}")

        # æ›´æ–°è¿›åº¦ - å¼€å§‹å¤„ç†
        if task_id in _task_storage:
            _task_storage[task_id]["progress"] = 30
            _task_storage[task_id]["status"] = "processing"
            _task_storage[task_id]["updatedAt"] = datetime.now().isoformat()

        # é¢„å¤„ç†é˜¶æ®µ - å¹¶è¡Œä¸‹è½½å’Œç¼“å­˜ç´ æ
        print("ğŸ“¥ ç¬¬ä¸€é˜¶æ®µ: å¹¶è¡Œé¢„å¤„ç†ç´ æ...")
        preprocess_start = time.time()

        # è¿™é‡Œå¯ä»¥æ·»åŠ å¹¶è¡Œç´ æä¸‹è½½é€»è¾‘
        # æš‚æ—¶ä½¿ç”¨åŸæœ‰çš„å¤„ç†é€»è¾‘ï¼Œä½†æ·»åŠ GPUä¼˜åŒ–

        if task_id in _task_storage:
            _task_storage[task_id]["progress"] = 50
            _task_storage[task_id]["updatedAt"] = datetime.now().isoformat()

        preprocess_time = time.time() - preprocess_start
        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {preprocess_time:.1f}ç§’")

        # è§†é¢‘åˆæˆé˜¶æ®µ - ä½¿ç”¨GPUåŠ é€Ÿ
        print("ğŸš€ ç¬¬äºŒé˜¶æ®µ: GPUåŠ é€Ÿè§†é¢‘åˆæˆ...")
        composition_start = time.time()

        # è°ƒç”¨ä¼˜åŒ–åçš„è§†é¢‘å¤„ç†æœåŠ¡
        result = await process_clips001(clip_req)

        composition_time = time.time() - composition_start
        print(f"âœ… GPUåˆæˆå®Œæˆï¼Œè€—æ—¶: {composition_time:.1f}ç§’")

        if task_id in _task_storage:
            _task_storage[task_id]["progress"] = 90
            _task_storage[task_id]["updatedAt"] = datetime.now().isoformat()

        # åå¤„ç†é˜¶æ®µ - ä¼˜åŒ–å’Œä¸Šä¼ 
        print("ğŸ“¤ ç¬¬ä¸‰é˜¶æ®µ: åå¤„ç†å’Œä¼˜åŒ–...")
        postprocess_start = time.time()

        # è¿™é‡Œå¯ä»¥æ·»åŠ è§†é¢‘ä¼˜åŒ–å’Œä¸Šä¼ é€»è¾‘

        postprocess_time = time.time() - postprocess_start
        print(f"âœ… åå¤„ç†å®Œæˆï¼Œè€—æ—¶: {postprocess_time:.1f}ç§’")

        # è®¡ç®—æ€»ä½“æ€§èƒ½ç»Ÿè®¡
        total_time = preprocess_time + composition_time + postprocess_time
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"   é¢„å¤„ç†: {preprocess_time:.1f}s ({preprocess_time/total_time*100:.1f}%)")
        print(f"   GPUåˆæˆ: {composition_time:.1f}s ({composition_time/total_time*100:.1f}%)")
        print(f"   åå¤„ç†: {postprocess_time:.1f}s ({postprocess_time/total_time*100:.1f}%)")
        print(f"   æ€»è€—æ—¶: {total_time:.1f}s")

        return result

    except Exception as e:
        print(f"âŒ GPUåŠ é€Ÿå¤„ç†å¼‚å¸¸: {e}")
        # å›é€€åˆ°åŸå§‹å¤„ç†æ–¹å¼
        print("ğŸ”„ å›é€€åˆ°CPUå¤„ç†æ¨¡å¼...")
        return await process_clips001(clip_req)

async def process_video_generation(task_id: str, clip_req: ClipRequest):
    """GPUåŠ é€Ÿçš„è§†é¢‘ç”Ÿæˆä»»åŠ¡å¤„ç†"""
    try:
        print(f"ğŸš€ å¼€å§‹GPUåŠ é€Ÿè§†é¢‘ç”Ÿæˆ: {task_id}")
        start_time = time.time()

        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        if not performance_optimizer.running:
            await performance_optimizer.start_monitoring()

        # å¯åŠ¨GPUè°ƒåº¦å™¨
        if not gpu_scheduler.running:
            await gpu_scheduler.start()

        # æ›´æ–°è¿›åº¦
        if task_id in _task_storage:
            _task_storage[task_id]["progress"] = 20
            _task_storage[task_id]["updatedAt"] = datetime.now().isoformat()

        # ä½¿ç”¨GPUåŠ é€Ÿçš„è§†é¢‘å¤„ç†æœåŠ¡
        result = await process_clips_with_gpu_acceleration(task_id, clip_req)

        processing_time = time.time() - start_time
        print(f"âœ… GPUåŠ é€Ÿå¤„ç†å®Œæˆ: {task_id}, è€—æ—¶: {processing_time:.1f}ç§’")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        if task_id in _task_storage:
            _task_storage[task_id]["status"] = "completed"
            _task_storage[task_id]["progress"] = 100
            _task_storage[task_id]["result"] = result
            _task_storage[task_id]["updatedAt"] = datetime.now().isoformat()
            
        # æ·»åŠ åˆ°æœ€æ–°ç”Ÿæˆè§†é¢‘åˆ—è¡¨
        _latest_generated_videos.append({
            "id": task_id,
            "result": result,
            "createdAt": datetime.now().isoformat()
        })
        
        # ä¿æŒæœ€æ–°10ä¸ªè§†é¢‘
        if len(_latest_generated_videos) > 10:
            _latest_generated_videos.pop(0)
            
    except Exception as e:
        print(f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
        if task_id in _task_storage:
            _task_storage[task_id]["status"] = "failed"
            _task_storage[task_id]["error"] = str(e)
            _task_storage[task_id]["updatedAt"] = datetime.now().isoformat()
        raise

def get_queue_status_info():
    """è·å–é˜Ÿåˆ—çŠ¶æ€ä¿¡æ¯"""
    queue_size = _task_queue.qsize()
    processing_count = len(_processing_tasks)
    available_workers = _max_concurrent_tasks - processing_count
    
    return {
        "queueSize": queue_size,
        "processingCount": processing_count,
        "maxConcurrentTasks": _max_concurrent_tasks,
        "availableWorkers": available_workers,
        "isProcessing": processing_count > 0,
        "processingTasks": list(_processing_tasks)
    }

# ä¿å­˜é¡¹ç›®é…ç½®
@router.post("/api/projects")
async def save_project_and_generate(req: ClipRequest, background_tasks: BackgroundTasks):
    # ä¿å­˜é¡¹ç›®é…ç½®åˆ°å†…å­˜å­˜å‚¨
    project_id = str(uuid4())
    _project_storage[project_id] = req
    
    # å¯åŠ¨é˜Ÿåˆ—å¤„ç†å™¨ï¼ˆå¦‚æœæœªå¯åŠ¨ï¼‰
    if not _queue_processor_started:
        background_tasks.add_task(start_concurrent_queue_processor)
    
    return {
        "success": True,
        "data": {
            "id": project_id,
            "name": req.name,
            "videos": [v.dict() for v in req.videos],
            "audios": [a.dict() for a in req.audios],
            "posters": [p.dict() for p in (req.posters or [])],
            "scripts": [s.dict() for s in req.scripts],
            "duration": req.duration,
            "videoCount": req.videoCount,
            "voice": req.voice,
            "voiceSpeed": req.voiceSpeed,
            "style": req.style.dict() if hasattr(req.style, "dict") else req.style,
            "portraitMode": req.portraitMode
        }
    }

# å¼€å§‹ç”Ÿæˆè§†é¢‘
@router.post("/api/generation/start")
async def start_generation(req: StartGenerationRequest, background_tasks: BackgroundTasks):
    # å¯åŠ¨é˜Ÿåˆ—å¤„ç†å™¨ï¼ˆå¦‚æœæœªå¯åŠ¨ï¼‰
    if not _queue_processor_started:
        background_tasks.add_task(start_concurrent_queue_processor)
    
    # ç”Ÿæˆä»»åŠ¡ID
    task_id = str(uuid4())
    
    # è·å–é¡¹ç›®é…ç½®
    if req.projectId not in _project_storage:
        return {
            "success": False,
            "error": "é¡¹ç›®ä¸å­˜åœ¨"
        }
    
    clip_req = _project_storage[req.projectId]
    
    # è·å–å½“å‰é˜Ÿåˆ—çŠ¶æ€
    queue_status = get_queue_status_info()
    queue_size = queue_status["queueSize"]
    processing_count = queue_status["processingCount"]
    
    # åˆ¤æ–­ä»»åŠ¡çŠ¶æ€
    if processing_count < _max_concurrent_tasks:
        # æœ‰ç©ºé—²å·¥ä½œåç¨‹ï¼Œç«‹å³å¤„ç†
        task_status = "processing"
        queue_position = 1
    else:
        # éœ€è¦æ’é˜Ÿ
        task_status = "queued"
        queue_position = queue_size + 1
    
    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    _task_storage[task_id] = {
        "id": task_id,
        "projectId": req.projectId,
        "status": task_status,
        "progress": 0,
        "result": None,
        "error": None,
        "createdAt": datetime.now().isoformat(),
        "updatedAt": datetime.now().isoformat(),
        "queuePosition": queue_position,
        "queueSize": queue_size + 1
    }
    
    # å°†ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
    await _task_queue.put({
        'task_id': task_id,
        'clip_req': clip_req
    })
    
    # è¿”å›ä»»åŠ¡ä¿¡æ¯
    status_msg = "å·²åŠ å…¥å¤„ç†é˜Ÿåˆ—" if task_status == "queued" else "å¼€å§‹å¤„ç†"
    
    print(f"ğŸ“ æ–°ä»»åŠ¡ {task_id}: {status_msg}")
    print(f"   é˜Ÿåˆ—çŠ¶æ€: é˜Ÿåˆ—ä¸­{queue_size + 1}ä¸ªä»»åŠ¡, æ’é˜Ÿä½ç½®#{queue_position}")
    print(f"   æ­£åœ¨å¤„ç†: {processing_count}/{_max_concurrent_tasks} ä¸ªä»»åŠ¡")
    
    return {
        "success": True,
        "data": {
            **_task_storage[task_id],
            "message": status_msg,
            "estimatedWaitTime": f"é¢„è®¡ç­‰å¾… {queue_position * 3} åˆ†é’Ÿ" if queue_position > 1 else "æ­£åœ¨å¤„ç†ä¸­"
        }
    }

# è·å–ä»»åŠ¡çŠ¶æ€
@router.get("/api/generation/status/{task_id}")
async def get_generation_status(task_id: str):
    if task_id not in _task_storage:
        return {
            "success": False,
            "error": "ä»»åŠ¡ä¸å­˜åœ¨"
        }
    
    task_data = _task_storage[task_id].copy()
    
    # å¦‚æœæ˜¯æ’é˜ŸçŠ¶æ€ï¼Œæ›´æ–°é˜Ÿåˆ—ä¿¡æ¯
    if task_data["status"] == "queued":
        current_queue_size = _task_queue.qsize()
        task_data["currentQueueSize"] = current_queue_size
        task_data["estimatedWaitTime"] = f"é¢„è®¡ç­‰å¾… {current_queue_size * 3} åˆ†é’Ÿ"
    
    return {
        "success": True,
        "data": task_data
    }

# GPUæ€§èƒ½ç›‘æ§æ¥å£
@router.get("/api/gpu/status")
async def get_gpu_status():
    """è·å–GPUçŠ¶æ€å’Œæ€§èƒ½ä¿¡æ¯"""
    try:
        # è·å–GPUè°ƒåº¦å™¨çŠ¶æ€
        gpu_status = gpu_scheduler.get_status() if gpu_scheduler.running else {
            "running": False,
            "gpu_tasks_running": 0,
            "cpu_tasks_running": 0,
            "stats": {"completed_tasks": 0, "failed_tasks": 0}
        }

        # è·å–æ€§èƒ½ä¼˜åŒ–å™¨çŠ¶æ€
        perf_status = performance_optimizer.get_current_status() if performance_optimizer.running else {
            "running": False,
            "stats": {"avg_gpu_utilization": 0, "avg_cpu_utilization": 0}
        }

        # è·å–é˜Ÿåˆ—çŠ¶æ€
        queue_status = get_queue_status_info()

        return {
            "success": True,
            "data": {
                "gpu_scheduler": gpu_status,
                "performance_optimizer": perf_status,
                "task_queue": queue_status,
                "gpu_optimization_enabled": _gpu_optimization_enabled,
                "max_concurrent_tasks": _max_concurrent_tasks,
                "current_processing_tasks": len(_processing_tasks)
            }
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"è·å–GPUçŠ¶æ€å¤±è´¥: {str(e)}"
        }

# GPUä¼˜åŒ–é…ç½®æ¥å£
@router.post("/api/gpu/config")
async def update_gpu_config(config: Dict[str, Any]):
    """æ›´æ–°GPUä¼˜åŒ–é…ç½®"""
    global _max_concurrent_tasks, _gpu_optimization_enabled

    try:
        if "max_concurrent_tasks" in config:
            new_max = int(config["max_concurrent_tasks"])
            if 1 <= new_max <= 10:  # åˆç†èŒƒå›´
                _max_concurrent_tasks = new_max
                print(f"ğŸ”§ æ›´æ–°æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°: {_max_concurrent_tasks}")

        if "gpu_optimization_enabled" in config:
            _gpu_optimization_enabled = bool(config["gpu_optimization_enabled"])
            print(f"ğŸ”§ GPUä¼˜åŒ–å¼€å…³: {'å¼€å¯' if _gpu_optimization_enabled else 'å…³é—­'}")

        return {
            "success": True,
            "data": {
                "max_concurrent_tasks": _max_concurrent_tasks,
                "gpu_optimization_enabled": _gpu_optimization_enabled
            }
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"æ›´æ–°GPUé…ç½®å¤±è´¥: {str(e)}"
        }

# è·å–é˜Ÿåˆ—çŠ¶æ€
@router.get("/api/generation/queue/status")
async def get_queue_status():
    queue_status = get_queue_status_info()
    queue_size = queue_status["queueSize"]
    processing_count = queue_status["processingCount"]
    
    # ç»Ÿè®¡å„çŠ¶æ€ä»»åŠ¡æ•°é‡
    queued_tasks = [t for t in _task_storage.values() if t.get("status") == "queued"]
    processing_tasks = [t for t in _task_storage.values() if t.get("status") == "processing"]
    completed_tasks = [t for t in _task_storage.values() if t.get("status") == "completed"]
    failed_tasks = [t for t in _task_storage.values() if t.get("status") == "failed"]
    
    return {
        "success": True,
        "data": {
            "queueSize": queue_size,
            "processingCount": processing_count,
            "maxConcurrentTasks": _max_concurrent_tasks,
            "availableWorkers": queue_status["availableWorkers"],
            "isProcessing": queue_status["isProcessing"],
            "statistics": {
                "queued": len(queued_tasks),
                "processing": len(processing_tasks),
                "completed": len(completed_tasks),
                "failed": len(failed_tasks),
                "total": len(_task_storage)
            },
            "estimatedWaitTime": f"æ–°ä»»åŠ¡é¢„è®¡ç­‰å¾… {(queue_size + 1) * 3} åˆ†é’Ÿ" if queue_size > 0 or processing_count >= _max_concurrent_tasks else "å¯ç«‹å³å¤„ç†"
        }
    }

# è·å–æœ€æ–°ç”Ÿæˆçš„è§†é¢‘
@router.get("/api/generation/latest")
async def get_latest_generated_videos():
    return {
        "success": True,
        "data": _latest_generated_videos[-10:]  # è¿”å›æœ€æ–°10ä¸ª
    }
